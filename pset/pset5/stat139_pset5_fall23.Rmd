---
title: "Problem Set 5: More Regression Modeling"
author: "Linh Vu (Collab: Brice Laurent)"
date: "Due: October 27, 2023"
output: pdf_document
---

\begin{small} 
		
\noindent This assignment is \textbf{due Friday, October 27 at 11:59pm}, handed in on Gradescope (remember, there are two separate submissions, one for your pdf, and another for you rmd file).  Show your work and provide clear, explanations when asked.  \textbf{Incorporate the \underline{relevant} R output in this R markdown file}. Only the key output should be displayed for each problem and the relevant parts should be \textbf{highlighted} in some way.  Make sure that you write-up any interpretation of R-code in your own words (don't just provide the output).

\normalsize

\vspace{0.1in}

\noindent \textbf{Collaboration policy (for this and all future homeworks)}: You are encouraged to discuss the problems with other students, but you must write up your solutions yourself and in your own words. Copying someone else's solution, or just making trivial changes is not acceptable. 
\vspace{0.1in}
		
\end{small}
	
```{r setup}
library(tidyverse)
```


**Problem 1.** 

Let $X \sim \text{Unif}(a,b)$.  Feel free to use results from the *Stat 110 Distribution Sheet* as seen on the midterm exam. 

```{r 1a}
set.seed(139)
a = -0.5
b = 0.5
x = runif(10^6, a, b)
x2 = x^2
cov(x, x2)

(a+b)*(a-b)^2/12
```


(a) Determine the covariance between $X$ and $X^2$.

\textcolor{blue}{Using the formula $Cov(X,Y) = E(XY)-E(X)E(Y)$, we get:
$$Cov(X,X^2) = E(X^3) - E(X)E(X^2)$$}

\textcolor{blue}{Find $E(X^2)$ using LOTUS: $E(X^2) = \int_{a}^{b}X^2\frac{1}{b-a}dX = \frac{b^3-a^3}{3(b-a)}$}

\textcolor{blue}{Find $E(X^3)$ using LOTUS:
$E(X^3) = \int_{a}^{b}X^3\frac{1}{b-a}dX = \frac{b^4-a^4}{4(b-a)}$}

\textcolor{blue}{Combining everything, we get:
$$Cov(X,X^2) = \frac{b^4-a^4}{4(b-a)} - \frac{a+b}{2}\frac{b^3-a^3}{3(b-a)}$$
$$=\frac{3(b^2-a^2)(b^2+a^2)-2(a+b)(b^3-a^3)}{12(b-a)}$$
Dividing both the numerator and denominator by $b-a$, we get:
$$=\frac{3(a+b)(b^2+a^2) - 2(a+b)(a^2+ab+b^2)}{12}$$
$$=\frac{(a+b)(3a^2+3b^2-2a^2-2ab-2b^2)}{12}$$
$$=\frac{(a+b)(a-b)^2}{12}$$}

(b) Assume $b-a=1$ (so that the variability of $X$ is fixed).  For what values of $a$ (and $b$) will this covariance be zero?  When will this covariance be large (and positive)?  When will it be negative (and large in magnitude)?  What does this mean for where the distribution of $X$ is centered in each case?

\textcolor{blue}{Using results from part (a), we know that the covariance in this case is $\frac{a+b}{12}$}

\textcolor{blue}{The covariance is 0 when $a+b=0$. And since $b-a=1$, we find that $a=-0.5$, $b=0.5$.}

\textcolor{blue}{The covariance is large and positive when $a+b$ is large and positive (i.e. a and b are both large and 1 unit apart). Similarly, the covariance is negative and large in magnitude when $a+b$ is negative and large in magnitude (i.e. a and b are both negative and large in magnitude). When $X$ is centered at a negative value far from 0, the covariance is very negative, and if the center is positive and far from 0, the covariance is large and positive, and if the center is 0, the covariance is 0.}

(c) What are the implications of the results in (b) for a quadratic regression model (when $X$ and $X^2$ are both used as predictors)? Is this phenomenon specific to the Uniform distribution?

*Note: do not be afraid to check your answers empirically using R.

\textcolor{blue}{Because $X$ and $X^2$ very rarely have covariance of 0 (i.e. not at all correlated), when $X$ and $X^2$ are both used as predictors, we face potential issues of multicollinearity. As a result, we need to be careful about including many polynomial terms. This phenomenon is not specific to the Uniform distirbution (see R chunk below).}

```{r 1c}
set.seed(139)
x = rnorm(10^6, 2, 4)
x2 = x^2
cov(x, x2)

x = rpois(10^6, 4)
x2 = x^2
cov(x, x2)
```


\newpage




**Problem 2.** 

The file 'pregnancydata.csv' includes several variables to model the birthweight of babies (measured through an online survey).  Those variables are defined below.  Use this data set in R to answer the questions below:
\vspace{-0.06in}

\small
\begin{tabular}[h]{rl}
\bf{id}: & a unique identifier of the mother\\
\bf{weight}: & birthweight of the newborn baby, in ounces \\
\bf{pregnancylength}: & the length of the pregnancy, in days \\
\bf{country}: & where the birth took place with categories United States (US), \\
& United Kingdom (UK), Canada (Can), and Other \\
\bf{motherage}: & age of mother at childbirth, in years\\
\bf{multiples}: & whether the baby was a 1=singleton or 2=twin\\
\bf{sex}: & sex of the baby: girl or boy\\
\bf{induced}: & a binary indicator for whether labor was induced with oxytocin\\
\bf{cesarean}: & a binary indicator for whether a cesarean (c-section) was performed\\
\bf{previousbirths}: & the number of births by the mother previous to this recorded one (from 0 to 10)\\
\end{tabular}
\normalsize

(a) Fit a regression model to predict weight from country and use the `relevel` command to make the "Other" group the reference group (call this \textbf{Model 1}).  Interpret the results and provide a visual to support your conclusions.  

```{r 2a}
pregnancy <- read.csv("data/pregnancydata.csv")
pregnancy$country = relevel(as.factor(pregnancy$country), "Other")

mod1 <- lm(weight~country, pregnancy)
summary(mod1)

x = c("Other", "Can", "UK", "US")
predict(mod1, new=data.frame(country=x))
plot(weight~country, pregnancy)
```

\textcolor{blue}{The intercept means that babies born in other countries weigh `r round(mod1$coefficients[1],3)` ounces on average. The other slope estimates mean the difference between average weigh of babies born in Canada, UK, US and those born in other countries. Specifically, compared to babies born in other countries, babies born in Canada weigh `r round(mod1$coefficients[2],3)` ounce more (this difference is significant due to small p-value); babies born in the UK weigh `r round(mod1$coefficients[3],3)` ounce more (this difference is not significant); babies born in the US weigh `r round(-mod1$coefficients[4],3)` ounce less (this difference is significant).}

\textcolor{blue}{The side-by-side boxplot shows that babies in Canada weigh slightly more on average, and babies in the US weigh slightly less on average.}

(b) Build a $3^{rd}$ order polynomial regression model to predict weight from `pregnancylength` (call this **Model 2**). Interpret the output and provide a visual to support the results of the model.  

```{r 2b}
mod2 <- lm(weight~poly(pregnancylength, 3, raw=T), pregnancy)
summary(mod2)

x = min(pregnancy$pregnancylength):max(pregnancy$pregnancylength)
yhat = predict(mod2, new=data.frame(pregnancylength=x))
plot(weight~pregnancylength, pregnancy)
lines(yhat~x,col="red",lwd=3)
```

\textcolor{blue}{$\hat{\beta_0}$: when length of the pregnancy is 0, the baby weighs `r round(mod2$coefficients[1],3)` ounces on average. Overall, all of the estimates are significant due to the small p-values, so adding higher-order polynomial terms provide significant result. The R-squared value of $0.26$ means that 26\% of the variability in baby's weight can be explained by this model. The plot shows that at after around day 150, the weight of the baby goes up (at an increasing rate) until around day 280, until it levels off at around day 300 onwards and decreases at around day 320.}

(c) Use **Model 2** to estimate the probability that a baby will weigh less than 7 pounds (112 ounces) when born on day 280. 

\textcolor{blue}{The probability that a baby will weigh less than 7 pounds when born on day 280 is 0.218. We got the standard error from the model output in part (b), and we used the formula $t^* = \frac{fit-observed}{SE} = \frac{123.96-112}{15.33} = 0.78$. We then used the \texttt{pt} function to get the probability of getting a value more extreme than $t^*$ from the $t$ distribution.}

```{r 2c}
# predict
new.data=data.frame(pregnancylength=280)
bound <- predict(mod2, new.data, interval="prediction")
bound

# calculate probability
SE = 15.33
t_star = (bound[1]-112)/SE
1-pt(t_star, df=9061)
```


(d) It is of medical interest to determine at what gestational age a developing fetus is gaining weight the fastest.  Use **Model 2** to estimate this *period of fastest growth*.

\textcolor{blue}{From model 2, we know that the line of best fit is 
$$\hat{weight} = 517 -7.86 \cdot length +3.65(10)^{-2} \cdot length^2 - 5.02(10)^{-5} \cdot length^3$$
We take the 2nd derivative with respect to length to find the period of fastest growth
$$2(3.65)(10)^{-2}-6(5.02)(10)^{-5}  \cdot length = 0$$
Solving for length gives us $length = 242.36$, meaning that the period of fastest growth is estimated as day 242 according to model 2.}

(e) Fit a LOESS model (call this **Model 3**) to predict weight from `pregnancylength` (use a smaller span of 0.3).   Provide a visual to support the results of the model.  How does this model compare to **Model 2** in its prediction accuracy?

\textcolor{blue}{The LOESS model and the 3rd degree polynomial model has similar R2 values (both approximately 0.26, the former has slightly higher R2 value). This means that within the range, the two models have similar prediction accuracy. But considering all the possible values (including outside the range), the LOESS model might have higher accuracy because it captures the true trend of baby's weight: it increases as the pregnancy progresses and perhaps plateaus/decreases if the pregnancy is abnormally long. On the other hand, the polynomial regression model has a weird/unrealistic curve that starts high and decreases over time before increasing at around day 150 (in reality, babies born prematurely do not weigh that much, and baby's weight increases over time)}

```{r 2e}
mod3 <- loess(weight~pregnancylength, pregnancy, span=0.3)
summary(mod3)

x=min(pregnancy$pregnancylength):max(pregnancy$pregnancylength)
yhat=predict(mod3, new=data.frame(pregnancylength=x))

plot(weight~pregnancylength, pregnancy, col=rgb(0.5, 0.5, 0.5, 0.2))
lines(yhat~x,col="red",lwd=4)

# calculate r2
# Predict the values using the model
predicted_values <- predict(mod3)

# calculate the total sum of squares (TSS)
tss <- sum((pregnancy$weight - mean(pregnancy$weight))^2)

# calculate the residual sum of squares (RSS)
rss <- sum((pregnancy$weight - predicted_values)^2)

# calculate R2 of model 3
1 - (rss / tss)

# r2 of model 2
0.2627
```


\newpage


**Problem 3.** 

In this problem, we will attempt to investigate whether the COVID-19 related restrictions imposed by the government had any effect on the reporting of criminal activity in the Boston Police Department (BPD).  We will be using the same combined dataset from last time (now named 'bpd.csv') that includes the number of daily incident reports filed (`count`) and various weather indicators on those days (`maxtemp` is the only weather variable we will use in this problem).  Note: we also used these data in Pset 3.

Note: a state of emergency was declared in Massachusetts on March 10, 2020, and restrictions on non-essential businesses, schools, and MBTA service were mainly put into effect on March 17,2020 (see this [City of Boston article](https://www.boston.gov/departments/public-health-commission/coronavirus-timeline) for the timeline). 

The \textsf{R} chunk below reads in the data and includes some code to create a variable called `dayinyear` in the `bpd` data frame that counts the number of days into the year, starting with 0 for Jan 1 (similar to what was done on the previous pset).

```{r 3 setup}
bpd = read.csv('data/bpd.csv')

jan1_19 = as.Date("1/1/19",format="%m/%d/%y")
jan1_20 = as.Date("1/1/20",format="%m/%d/%y")
jan1_21 = as.Date("1/1/21",format="%m/%d/%y")

bpd$dayinyear = as.Date(bpd$date,format="%m/%d/%y") - jan1_19 
bpd$dayinyear[bpd$year==2020] =
  as.Date(bpd$date,format="%m/%d/%y")[bpd$year==2020] - jan1_20 
bpd$dayinyear[bpd$year==2021] =
  as.Date(bpd$date,format="%m/%d/%y")[bpd$year==2021] - jan1_21
```

(a) Create a binary/dummy variable (call it `restrictions`) to indicate whether that day falls under the time period of state of emergency or restricted business operations in the city of Boston (all dates between and including March 10, 2020 and Friday, May 28, 2020).  How many days fall in this time period in the data set?

\textcolor{blue}{80 days in the data set fall in this time period.}

```{r 3a}
start = as.Date("03/10/2020", format="%m/%d/%y") - jan1_20
end = as.Date("05/28/2020", format="%m/%d/%y") - jan1_20

bpd$restrictions <- 1*(bpd$dayinyear >= start & bpd$dayinyear <= end & bpd$year == 2020)

sum(bpd$dayinyear >= start & bpd$dayinyear <= end & bpd$year == 2020)
```


(b) Calculate the mean number of daily incident reports filed by the BPD during the restriction orders in Boston.  Separately calculate the mean number of daily incident reports for a comparison group with the same calendar dates in the pre-pandemic portion of the data. Use these two groups to calculate a reasonable 95% confidence interval for the effect of COVID-19 restrictions on the reporting of crime in the BPD (based on a simple 2-group comparison method and not linear regression).

```{r 3b}
bpd20 <- bpd[bpd$year == 2020 & bpd$dayinyear >= start & bpd$dayinyear <= end, c("year", "count")]
bpd19 <- bpd[bpd$year == 2019 & bpd$dayinyear >= start & bpd$dayinyear <= end, c("year", "count")]

t.test(count~year, rbind(bpd20, bpd19))
```

\textcolor{blue}{The mean of daily incident reports during restriction period in 2020 is 159.125, and that value for 2019 is 264.7. Conducting the 2-sample $t$ test, we get a 95\% CI for the effect of restrictions on reporting of crime of (95.96, 115.19). This means that COVID restrictions tend to correlate with a decrease of between 95.96 and 115.19 daily crimes reported.}

(c) Fit a  linear regression model to predict `count` from `maxtemp` and `restrictions` (call it **model1**), and print out the `summary` results. Briefly interpret the coefficient estimates and use this model to estimate the effect of COVID-19 restrictions on the reporting of crime in the BPD (with 95% confidence).

```{r 3c}
model1 <- lm(count ~ maxtemp + restrictions, bpd)
summary(model1)

confint(model1, "restrictions", type="confidence")
```

\textcolor{blue}{$\hat{\beta_0}$: when \texttt{maxtemp} is 0F and when there is no restriction, the expected number of daily reported crimes is `r round(coefficients(model1)[1],2)` in Boston.}

\textcolor{blue}{$\hat{\beta_1}$: the expected number of daily reported crimes increases by  `r round(coefficients(model1)[2],2)` as \texttt{maxtemp} increases by 1F, if we consider the same type of day (restriction vs non-restriction).}

\textcolor{blue}{$\hat{\beta_2}$: \texttt{maxtemp} is held constant, the difference between expected number of daily reported crimes between restriction and non-restriction day is `r round(-coefficients(model1)[3],2)` (restriction days have fewer crimes).}

\textcolor{blue}{All the coefficients are significant according to the model output.}

\textcolor{blue}{The 95\% confidence interval for the effect of COVID-19 restrictions is (-70.34, -51.02). Since 0 is not included in the interval, this means that restriction rules decrease the number of daily crimes in Boston.}

(d) Fit a linear regression model to predict `count` from `maxtemp`, `restrictions`, `dayinyear` and all 2-way interactions between these 3 predictors (call it **model2**), and print out the `summary` results.  Interpret what this model says about the relationship between crime reporting in the BPD and COVID-19 restrictions. Compute an estimate and 95\% CI for the effect of restrictions on the 0th day of the year, assuming a \texttt{maxtemp} of 0 degrees.  Also estimate `count` (and provide a 95\% CI) on the 91st day of the year in 2020, assuming the temperature was 50 degrees. Do the same for 2019 and compare the difference.

```{r 3d}
model2 <- lm(count~(maxtemp + restrictions + dayinyear)^2, bpd)
summary(model2)

new.data1 <- data.frame(dayinyear=as.difftime(91, unit="days"), maxtemp=50, restrictions=0)
new.data2 <- data.frame(dayinyear=as.difftime(91, unit="days"), maxtemp=50, restrictions=1)

predict(model2, new.data1, interval="prediction")
predict(model2, new.data2, interval="prediction")
```

\textcolor{blue}{From the model output, we know that considering two days, one with restriction and one without, with the same \texttt{maxtemp} of $a$ and the same day in year $b$, the difference in daily crime count is $-78.07 + 0.913a -2.911b$.}

\textcolor{blue}{The model output shows that the effect of restrictions on the 0th day of the year, assuming 0 degrees, is -0.7807 (this is the same as considering the 2-sample $t$ test for restriction=1 vs restrictions=0). This estimate is significant because the p-value of 0.0416 is smaller than 0.05.}

\textcolor{blue}{95\% CI of \texttt{count} on the 91st day of the year in 2020, assuming the max temperature was 50 degrees is (73.60, 240.86), and the same 95\% CI for 2019 is (133.23, 299.05). These two intervals are overlapping, but the interval for 2019 (non-restriction year) is higher than the interval for 2020 (restriction year), so it is likely that restriction is correlated with a decrease in daily count of crime.}

(e) Perform a formal hypothesis test to determine whether **model2** performs significantly better at predicting `count` than **model1**.

```{r 3e}
anova(model1, model2)
```

\textcolor{blue}{We conducted an SSE F-test to compare the 2 models because they have similar structures and one is nested within the other. The null hypothesis is that adding more terms to the original model does not add predictive power, and the alternative hypothesis is that at least one term, when added to the original model, increases the predictive power. The F-statistic is 1.044, with df = 4, 1089. The p-value is 0.3834, so we can retain the null hypothesis and conclude that model 2 does not perform significantly better than model 1.}

(f) Investigate the assumptions for **model2**. Be sure to include and reference useful visuals.

```{r 3f}
plot(model2, which=c(1,2))
```

\textcolor{blue}{The assumptions are: (1) Indepedence: the observations are not independent because the daily count of crime on neighboring days during COVID restriction would be more similar to each other than the daily count of crime during non-restriction period. (2) Linearity: this assumption is met because the points are similarly above the horizontal line and below the horizontal line in the 1st plot. (3) Normality: this assumption is met because the standardized residuals more or less follow the theoretical quantiles in the QQ plot (and we have a large dataset so we can apply the CLT). (4) Homoskedasticity: this seems to be violated because the variance of residuals increases as the fitted values increase.}

(g) Determine which 4 dates **model2** did the worst job at predicting `count`. Can you think of a reason why any of these dates do not follow the relationships in this model? (all 4 are explainable with a little Google searching)

```{r 3g}
bpd$resids <- abs(model2$residuals)
bpd %>%
  slice_max(n=4, order_by=resids)
```

\textcolor{blue}{12/14/21 and 12/25/21 were Christmas holidays, and crime rates tend to go up during holiday time due to a variety of reasons such as increased alcohol asumption and heightened economic stress. There was a Knicks vs Celtics game on 11/1/19, so the high frequency of crime in Boston might have been related to this game. On 5/15/19, a police captain was placed on leave, so this might have affected the relationships in the model.}

(h) Write a 200-300 word summary addressing whether there is evidence that COVID-19 reduced the amount of crime in Boston.  Be sure to reference the results above (specifically, which approach you think was most reasonable) and mention any biases or confounders that may be present in this relationship.

\textcolor{blue}{Model 1 is simpler and only includes 2 predictors, whereas model 2 is more complex and includes all 2-way interactions among 3 predictors. In the model 2's output, we see that all of the estimates for coefficients that involve \texttt{dayinyear} are insignificant (due to large p-values). Also, the ESS F-test conducted in part (e) leads us to a similar conclusions: adding more terms to the model (\texttt{dayinyear} and 2-way interaction terms) do not improve the model's predictive power. Therefore, I think model 1 is the more reasonable approach for 2 reasons: it is simpler, and a more complex model doesn't necessarily improve the predictive power. Based on model 1's output, the estimate associated with \texttt{restrictions} is -60.678, meaning that holding temperature constant, on average, a day with restriction has 61 fewer reported crimes than a day without restriction. This is significant evidence that COVID-19 restriction is associated with fewer reported crimes. We can't draw a causal link because this is not a randomized control trial. Because of its simplicity, model 1 might fail to capture certain confounders; for example, during COVID restrictions period, the government might have more measures to alleviate economic pressures, leading to fewer reported crimes.}

\newpage


**Problem 4.** 

Perform a simulation study (with 1,000 iterations) where the data are **generated** from the following $\sin$ function:
$$Y_i = \sin(X_i) + \varepsilon_i$$

where $\varepsilon_i \sim N(0,\sigma^2=0.1^2)$ and independent, and $X_i$ are sampled independently from a Unif($a=0,b=6$), for $n=50$ observations.

For each iteration, fit 4 different polynomial models (use the raw form in R): (i) $3^{rd}$ order, (ii) $5^{th}$ order, (iii) $7^{th}$ order, and (iv) $=9^{th}$ order.  Save the $\beta_1$ coefficient (linear term) estimates for each of the 4 models for each of the 1,000 iteration (presumably a 1000x4 matrix) and either separately save all 10 $\beta$ coefficient estimates for the $9^{th}$ order or the model objects themselves (in a list).

Evaluate which model is *best* in each iteration two ways: (i) based on sequential ESS $F$-tests (you will perform 3 of them in each iteration) and (ii) out-of-sample mean squared error (based on a single test set of $n_{test}=1000$ generated from the same data generating process as the regular $n=50$ set of observations...this does not need to be recreated in each iteration).

```{r 4a, warning = FALSE, message = FALSE}
# params
set.seed(139)
nsims <- 1000
ntest <- 1000

# beta1 estimates
beta1 <- data.frame(m1 = as.numeric(),
                    m2 = as.numeric(),
                    m3 = as.numeric(),
                    m4 = as.numeric())
m4_list <- list()

# best model
best_model_ess <- rep(NULL, nsims)
best_model_mse <- rep(NULL, nsims)

# test data
x_test <- runif(ntest, 0, 6)
epsilon_test <- rnorm(ntest,0,0.1)
y_test <- sin(x_test) + epsilon_test

# simulate
for(i in 1:nsims){
  
  n <- 50
  x <- runif(n, 0, 6)
  epsilon <- rnorm(n,0,0.1)
  y <- sin(x) + epsilon
  
  # create lm
  m1 <- lm(y~poly(x,3,raw=T))
  m2 <- lm(y~poly(x,5,raw=T))
  m3 <- lm(y~poly(x,7,raw=T))
  m4 <- lm(y~poly(x,9,raw=T))
  
  # save beta1 estimates
  beta1[i,1] <- coefficients(m1)[2]
  beta1[i,2] <- coefficients(m2)[2]
  beta1[i,3] <- coefficients(m3)[2]
  beta1[i,4] <- coefficients(m4)[2]
  
  # save m4
  m4_list[[i]] <- m4
  
  # determine best model using out-of-sample MSE
  yhat1 <- predict(m1, new=data.frame(x=x_test))
  yhat2 <- predict(m2, new=data.frame(x=x_test))
  yhat3 <- predict(m3, new=data.frame(x=x_test))
  yhat4 <- predict(m4, new=data.frame(x=x_test))
  
  mse <- c(mean((y_test-yhat1)^2),
           mean((y_test-yhat2)^2),
           mean((y_test-yhat3)^2),
           mean((y_test-yhat4)^2))

  best_model_mse[i] <- which.min(mse)
  
  # sequential ESS F tests
  test1 <- anova(m1, m2)[2,6]
  test2 <- anova(m2, m3)[2,6]
  test3 <- anova(m3, m4)[2,6]

  # determine best model using ESS F tests
  if(test1 < 0.05 & test2 < 0.05 & test3 < 0.05){
    best_model_ess[i] = 4
  } else if(test1 < 0.05 & test2 < 0.05){
    best_model_ess[i] = 3
  } else if(test1 < 0.05){
    best_model_ess[i] = 2
  } else(best_model_ess[i] = 1)
}

table(best_model_ess)
table(best_model_mse)
```


(a) Based on the ESS $F$-tests, how often is each of the 4 models considered the best?  Based on out-of-sample mean squared error?

\textcolor{blue}{Based on the ESS $F$-tests, model1 performs the best 9.3\% of the time, model2 performs the best 85.7\% of the time, model3 4.8\% of the time, and model4 0.02\% of the time.}

\textcolor{blue}{Based on the MSE method, model1 performs the best 1.5\% of the time, model2 performs the best 84.8\% of the time, model3 12.1\% of the time, and model4 1.6\% of the time.}

(b) Which metric is more conservative when it comes to overfitting?  How do you know?

\textcolor{blue}{The ESS $F$-test is more conservative to overfitting because it tends to fit simple models (lower-order polynomials) more of the time, whereas the MSE method determines 3rd and 4th order polynomials as the best models more often.}

(c) Plot 10+ $\hat{\mu}_Y$ curves (the predicted curve) based on the estimated $9^{th}$ order polynomial model (for 10+ iterations): with at least 5 curves for when $9^{th}$ order model wins and at least 5 curves for when it does not win (based on out-of-sample mean square error).  Be sure to color code these curves based on when this model wins vs. when it does not win.  Interpret this plot: what does this say about how overfitting affects out-of-sample mean square error?

```{r 4c}
# get model ids
win_id <- which(best_model_mse == 4)
lose_id <- which(best_model_mse == 1)

# plot
x_4c <- seq(0, 6, 0.01)
mu_hat <- predict(m4_list[[137]], new=data.frame(x=x_4c))
plot(mu_hat~x_4c, col="green", lwd=0.5)

for(i in win_id){
  mu_hat <- predict(m4_list[[i]], new=data.frame(x=x_4c))
  lines(mu_hat~x_4c, col="green", lwd=0.5)
}
for(i in lose_id){
  mu_hat <- predict(m4_list[[i]], new=data.frame(x=x_4c))
  lines(mu_hat~x_4c, col="red", lwd=0.5)
}
```

\textcolor{blue}{We expect the lines to follow the $sin(x)$ line with small variation. When model 4 wins, the plot behaves as expected (see the green lines), but when model 4 loses, the plot behaves weirdly (see the red lines and how they go way off near the boundaries). The training data includes more points in the middle of the range, so lost model 4 tends to behave badly outside of the range or at the boundaries. In general, this shows that overfitting (considering model 4 when it loses) leads to higher out-of-sample MSE.}

(d) Provide the boxplots of $\hat{\beta}_1$ estimates in each of the 4 models (should be a side-by-side boxplot with 4 boxplots based on 1000 estimates each).  Interpret this plot in context of this situation.  What does this illustrate?  Why is this not surprising?

```{r 4d}
boxplot(beta1)
```
\textcolor{blue}{As we fit more complex models, there are more outliers in the estimates of $\beta_1$. This illustrates that when we fit higher-order polynomials, we tend to get really varied estimates for the same coefficient. This is not surprising because the variance of all betas estimates is $\sigma^2(X^TX)^{-1}$ and it is large when we have lots of not-so-meaningful predictors. We estimate $sigma^2$ by calculating $\hat{\sigma}^2 = \frac{\sum(Y_i-\hat{Y})^2}{n-(p+1)}$. When a complex model doesn't improve $\sum(Y_i-\hat{Y})^2$, its $\hat{\sigma}^2$ increases because $n-(p+1)$ becomes smaller with more predictors in the model.}
