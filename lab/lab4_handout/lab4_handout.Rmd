---
title: "Simple Regression and Binary Predictors"
author: "Lab 4 Handout"
date: "Statistics 139"

fontsize: 11pt
geometry: margin=1in 

output:
  pdf_document:
    fig_width: 5
    fig_height: 3.5
---


\begin{small}
	
	\textbf{Topics}
	\begin{itemize}
    \item Simple Regression Inference
    \item Regression diagnostics
	  \item Categorical predictors with two levels
	  \item Understanding $R^2$
	\end{itemize}
	
\end{small}


#### Question 1: Simple linear regression

The Prevention of REnal and Vascular END-stage Disease (PREVEND) study took place between 2003 and 2006 in the Netherlands. Clinical and demographic data for the 4,095 participants were collected. The data are provided to you in the \texttt{prevend.csv} data file. 

As adults age, cognitive function changes, largely due to various cerebrovascular and neurodegenerative changes. The Ruff Figural Fluency Test (RFFT) is one measure of cognitive function; higher scores indicate better cognitive function. We will use linear regression to explore the relationship between age and RFFT score.
    
a) Create a scatterplot of RFFT score versus age and add a line of best fit. 
    
```{r 1a, fig.height = 4, fig.width = 5}
prevend=read.csv("data/prevend.csv")

# create a plot, fit a model, and add the fitted line
model_1a <- lm(RFFT ~ Age, data=prevend)
summary(model_1a)
plot(prevend$RFFT ~ prevend$Age)
abline(model_1a)
```

b) What are the slope and intercept values of the line of best fit? Interpret the values in the context of the data. Is the intercept value meaningful for this context?

\textcolor{blue}{Slope = `r model_1a$coefficients[1]`, and the intercept = `r model_1a$coefficients[2]`. The slope means that on average, when someone becomes one year older, their RFFT reduces by 1.17. The intercept means that when someone is 0 years old, their RFFT value on average is 132.34 (this isn't meaningful because extrapolation and babies aren't the smartest).}
    
\vspace{2cm}
    
c) On average, how much does mean RFFT score differ between an individual who is 60 years old versus an individual who is 50 years old?

```{r 1c}
model_1a$coefficients[2]*(60-50)
```

    
\vspace{1cm}
    
d) Is it valid to use the linear model to estimate average RFFT score for an individual who is 20 years old? Explain your answer.

\textcolor{blue}{Not really because the min age is 35 in the dataset. This would be extrapolation.}

```{r 1d}
summary(prevend$Age)
```

\vspace{1cm}
    

e) Formally test whether RFFT score is linearly associated with age. Calculate the associated 95\% confidence interval and interpret the interval.

\textcolor{blue}{small p-value --> reject null hypothesis and conclude there's a linear association between RFFT score and age. the confidence interval for the slope is entirely negative, so the association is negative.}

```{r 1e}
# the `summary` and `conf.int` commands on your `lm` object will be useful
summary(model_1a)
confint(model_1a)
```
\vspace{1cm}
    
f) Calculate a 95\% confidence interval for the mean RFFT score of individuals who are 60 years old. Interpret the interval.
    
```{r 1f}
# the `predict` command on your `lm` object will be useful
newdata <- data.frame(Age = 60)
predict(model_1a, newdata = newdata, interval = "confidence", level=0.95)
```
\vspace{1cm}
    
g) Calculate a 95\% prediction interval for the RFFT score of a future individual (i.e., one who is not in the dataset) who is 60 years old. Interpret the interval.

```{r 1g}
# the `predict` command on your `lm` object will again be useful
predict(model_1a, newdata = newdata, interval = "prediction", level=0.95)
```    
\vspace{1cm}
    
h) Construct a plot of RFFT score versus age that shows the line of best fit, the 95\% confidence bands, and the 95\% prediction bands.
    
```{r 1h}
#plot the data and line of best fit from before
plot(prevend$RFFT ~ prevend$Age)
abline(model_1a)

#plot confidence bands, you'll want to define a `dummy` x first
new_x <- seq(min(prevend$Age), max(prevend$Age))
conf.band <- predict(model_1a, newdata = data.frame(Age = new_x), interval = "confidence")
lines(new_x, conf.band[, 2])
lines(new_x, conf.band[, 3])

#plot prediction bands
pred_x <- seq(min(prevend$Age), max(prevend$Age))
conf.band <- predict(model_1a, newdata = data.frame(Age = pred_x), interval = "prediction")
lines(pred_x, conf.band[, 2])
lines(pred_x, conf.band[, 3])

```   

\newpage

#### Question 2: Regression diagnostics

There are five assumptions that must be met for a linear model to be considered reasonable: existence, linearity, independence, homogeneity of variance, and normally distributed residuals. Three of these can be checked.

Even though linearity and constant variability can be assessed from the scatterplot of $y$ versus $x$, it is helpful to consult residual plots for a clearer view. Normality of residuals is best assessed through a normal probability plot; although skew can be visible from a histogram of the residuals, deviations from normality are more obvious on a normal probability plot.

a) Create a residual plot where the residual values  (call them `prevend.residuals`) are plotted on the $y$-axis against predicted values  (call them `prevend.predicted`)  from the model on the $x$-axis, using data in \texttt{prevend}. Based on the plot, comment on the validity of the linearity and constant variability assumptions.
    
```{r 2a}
# residual plot. 
prevend.residuals <- model_1a$residuals
prevend.predicted <- model_1a$fitted.values
plot(prevend.residuals ~ prevend.predicted)
```

\vspace{1cm}

b) Run the code chunk below to create a normal probability plot of the residuals. For comparison purposes, the following figure shows a histogram of the residual values overlaid with a normal curve and the normal probability plot. Do the residuals seem approximately normally distributed?

```{r, fig.height = 4, fig.width = 8, echo = FALSE, message = FALSE, eval = FALSE}
par(mfrow = c(1, 2))

# histogram of residuals with fitted normal curve
hist(prevend.residuals, freq = FALSE, main = "Histogram of Residuals", 
     xlab = "Residuals", col = "turquoise") #freq = FALSE rescales histogram to have area 1
x = seq(min(prevend.residuals), max(prevend.residuals), 0.01)
y = dnorm(x, mean(prevend.residuals), sd(prevend.residuals))
lines(x, y, lwd = 1.5, col = "red")

# normal probability plot
qqnorm(prevend.residuals, cex = 0.75, col = "turquoise")
qqline(prevend.residuals)
```

\vspace{1cm}

c) Use a resampling approach to calculate a 95\% confidence interval for $\beta_1$. Compare this interval to the one in problem 1.
 
   
```{r}
# set parameters and seed
set.seed(139)
nboots <- 1000
n = nrow(prevend)
beta1_container = rep(NA, nboots)

# bootstrapping
for(i in 1:nboots){
  
  boot.index = sample(1:n, replace=T)
  boot.df = prevend[boot.index,]
  boot.lm = lm(RFFT~Age, boot.df)
  beta1_container[i] = coef(boot.lm)["Age"]
  
}

# determine the confidence interval
ci = quantile(beta1_container, c(0.025, 0.975))
ci

# visualize distribution of sampling statistic with ci added
hist(beta1_container)
abline(v = ci, col="red")
```    

\newpage

#### Question 3: Categorical predictors with two levels

Statins are a class of drug widely used to lower cholesterol. However, some physicians have raised the question of whether treatment with a statin might be associated with an increased risk of cognitive decline.

Statin use is coded as a factor, where \texttt{Nonuser} represents a non-user and \texttt{User} represents a user.
    
a) Create a visual showing the association between RFFT score and statin use. Describe what you see. Calculate the mean RFFT score in each statin use group.

```{r 3a}
# plot
boxplot(RFFT~Statin, data=prevend)

# calculate means
tapply(prevend$RFFT, prevend$Statin, mean)
```    

\vspace{1cm}
    
b) Fit a simple regression model. Calculate and interpret the slope coefficient, along with the associated 95\% confidence interval.
    
```{r 3b}
# fit model
lm2 <- lm(RFFT ~ Statin, data=prevend)

# model summary
summary(lm2)

# confidence interval
confint(lm2)
```    
c) Write the equation of the least-squares line and solve for the two possible values of $\widehat{RFFT}$. Confirm that the values match the ones from part (a).

```{r 3c}
lm2$coefficients[1]
lm2$coefficients[1] + lm2$coefficients[2]
```


\vspace{2.5cm}
    
    
d) Conduct a $t$-test for the difference in mean RFFT score between statin users and non-users. Compare the results of inference based on the linear model to those based on a two-group test.
   
```{r 3d}
#t-test
t.test(RFFT ~ Statin, data=prevend)
t.test(RFFT ~ Statin, data=prevend, var.equal=T)
```

\newpage

#### Question 4: Understanding $R^2$

The quantity $R^2$ describes the amount of variation in the response variable that is explained by the least squares line:

\[R^2 = \dfrac{\text{variance of predicted $y$-values}}{\text{variance of observed $y$-values}} = \dfrac{\text{Var}(\hat{y}_i)}{\text{Var}(y_i)} \]

$R^2$ can also be calculated using the following formula:

\[R^2 = \dfrac{\text{variance of observed $y$-values} - \text{variance of residuals} }{\text{variance of observed $y$-values}} = 1-\dfrac{ \text{Var}(e_i)}{\text{Var}(y_i)}\]

A simulation can be conducted in which $y$-values are sampled according to a population regression model $y = \beta_0 + \beta_1x + \epsilon$, where the parameters $\beta_0$, $\beta_1$, and the standard deviation of $\epsilon$ are known. Recall that $\epsilon$ is a normally distributed error term with mean 0 and standard deviation $\sigma$.

a) Simulate 100 $(x, y)$ values, where the values for $x$ are 100 numbers randomly sampled from a standard normal distribution and the values for $y$ are determined by the population model $y_i = 100 + 25x_i + \epsilon_i$, where $\epsilon_i \sim N(0, 5^2)$. Create a scatterplot of \texttt{y} versus \texttt{x} and add the line of best fit to the plot. 

```{r}
#set the seed

#simulate values

#plot the data with line of best fit

```

i. Does the line appear to be a good fit to the data?
        
\vspace{1cm}
    
ii. Why do the data points not fall exactly on a line, even though the data are simulated according to a known linear relationship between $x$ and $y$?
        
\vspace{1cm}
        
iii. How well does the regression line estimate the population parameters $\beta_0$ and $\beta_1$?
        
```{r}
#print model coefficients

```
 
        
iv. From a visual inspection, does it seem that the $R^2$ for this linear fit is relatively high or relatively low?

\vspace{1cm}
    
v. Using graphical summaries, compare the variances of the predicted and observed $y$-values; do they seem to have similar spread?
    
```{r}
#plots


```


vi. Calculate the $R^2$ of the model.

```{r}
#calculate R^2

```

\vspace{1cm}


b) Simulate 100 new $(x, y)$ values. Like before, the $x$ values are 100 numbers randomly sampled from a standard normal distribution and the $y$ values are determined by the population model $y_i = 100 + 25x_i + \epsilon_i$. For these data, however, the error term is distributed $N(0, 50^2)$.

```{r}
#clear the workspace
rm(list = ls())

#set the seed

#simulate values

```

i. Create a scatterplot of \texttt{y} versus \texttt{x} and add the line of best fit to the plot. Does the line appear to be a good fit to the data? How well does the regression line estimate the population parameters $\beta_0$ and $\beta_1$?

```{r}
#plot the data with line of best fit


```

\vspace{2cm}
    
ii. Using graphical summaries, compare the variances of the predicted and observed $y$-values; do they seem to have similar spread?

```{r}
#plots


```
 
\vspace{1cm}

iii. Based on the answers to parts (i) and (ii), do you expect the $R^2$ for this linear model to be relatively high or relatively low?

\vspace{1cm}

iv. Calculate the $R^2$ of the model.

    
```{r}
#calculate R^2

```


c) Run the code chunk below to simulate 100 new $(x, y)$ values based on a different true data-generative model. 

```{r}
#clear the workspace
rm(list = ls())

#set the seed
set.seed(2020)

#simulate values
n  = 100
x = rnorm(n)
error = rnorm(n, 0, 5)
y = 100 + 25*x + 5*x^2 + error
```

i. Fit a linear model predicting $y$ from $x$ to the data and calculate the $R^2$ for the model. Based on $R^2$, does the model seem to fit the data well?

    
```{r}
#calculate R^2

```
 
\vspace{2cm}

ii. Plot the data and add the line of best fit. Evaluate whether the linear model is a good fit to the data; how does viewing the data change the conclusion from part (i)?

```{r}
#plot the data

```

\vspace{1cm}